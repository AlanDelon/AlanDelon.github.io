{"posts":[{"title":"Redis-持久化及内存淘汰机制","content":"Redis提供了两种持久化功能，RDB持久化、AOF（Append Only File）持久化功能；内存淘汰机制，Redis服务器实际使用的是惰性删除和定期删除两种策略。 一、持久化机制 Redis是内存数据库，它将自己的数据库状态储存在内存里面，如果不想办法将储存在内存中的数据库状态保存到磁盘里面，那么一旦服务器进程退出，服务器中的数据库状态也会消失不见。为了解决这个问题，Redis提供了两种持久化功能，RDB持久化、AOF（Append Only File）持久化功能。 1.1 RDB持久化 命令： 使用SVAE 、BGSAVE命令可以生成RDB文件，将数据库状态存储到磁盘。 SAVE命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在服务器进程阻塞期间，服务器不能处理任何命令请求： redis&gt; SAVE // 等待直到RDB文件创建完毕 OK BGSAVE命令会派生出一个子进程，然后由子进程负责创建RDB文件，服务器进程继续处理请求 redis&gt; BGSAVE // 派生子进程，并由子进程创建RDB文件 Background saving started 定期自动保存： 通过设置服务器配置的save选项，让服务器每隔一段时间自动执行一次BGSAVE命令。用户可以设置多个保存条件，只要要其中任意一个条件被满足，服务器就会执行BGSAVE命令。 save 900 1 // 服务器在900秒之内，对数据库进行了至少1次修改 save 300 10 // 服务器在300秒之内，对数据库进行了至少10次修改 save 60 10000 // 服务器在60秒之内，对数据库进行了至少10000次修改 自动执行数据RDB持久化功能用到的字段定义，如下： struct redisServer { // ... struct saveparam *saveparams; /* Save points array for RDB */ long long dirty; /* Changes to DB from the last save */ time_t lastsave; /* Unix time of last successful save */ // ... } Redis的服务器周期性操作函数serverCron默认每隔100毫秒就会执行一次，该函数用于对正在运行的服务器进行维护，它的其中一项工作就是检查save选项所设置的保存条件是否已经满足，如果满足的话，就执行BGSAVE命令。如下： /* If there is not a background saving/rewrite in progress check if * we have to save/rewrite now. */ for (j = 0; j &lt; server.saveparamslen; j++) { struct saveparam *sp = server.saveparams+j; /* Save if we reached the given amount of changes, * the given amount of seconds, and if the latest bgsave was * successful or if, in case of an error, at least * CONFIG_BGSAVE_RETRY_DELAY seconds already elapsed. */ if (server.dirty &gt;= sp-&gt;changes &amp;&amp; server.unixtime-server.lastsave &gt; sp-&gt;seconds) { serverLog(LL_NOTICE,&quot;%d changes in %d seconds. Saving...&quot;, sp-&gt;changes, (int)sp-&gt;seconds); rdbSaveBackground(server.rdb_filename,rsiptr); break; } } 1.2 AOF持久化 原理： AOF持久化保存数据库状态的方法是将服务器执行的SET、SADD、RPUSH等命令保存到AOF文件中。被写入AOF文件的所有命令都是以Redis的命令请求协议格式保存的纯文本，比较易读。 AOF持久化功能的实现可以分为命令追加（append）、文件写入、文件同步（sync）三个步骤。 当AOF持久化功能处于打开状态时，服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的aof_buf缓冲区的末尾。是否进行文件写入、文件同步则通过一下参数控制： # appendfsync always appendfsync everysec # appendfsync no 当appendfsync的值为always时，每个事件循环都要将aof_buf缓冲区中的所有内容写入到AOF文件，并且同步AOF文件，效率最慢但是最安全的，即使故障停机也只会丢失一个事件循环中所产生的命令数据； 当appendfsync的值为everysec时，每个事件循环都要将aof_buf缓冲区中的所有内容写入到AOF文件，并且每隔一秒对AOF文件进行一次同步。从效率上来讲，everysec模式足够快，故障停机会只丢失一秒的命令数据。 当appendfsync的值为no时，每个事件循环都要将aof_buf缓冲区中的所有内容写入到AOF文件，何时对AOF文件进行同步，则由操作系统控制。该模式下速度是最快的，这种模式会在系统缓存中积累一段时间的写入数据，单次同步时长通常是三种模式中时间最长的。从平摊操作的角度来看，no模式和everysec模式的效率类似，故障停机时将丢失上次同步AOF文件之后的所有写命令数据。 AOF重写 AOF持久化是通过保存被执行的写命令来记录数据库状态的，所以随着服务器运行时间的流逝，AOF文件会越来越大，如果不加以控制的话，体积过大的AOF文件很可能对Redis服务器、甚至整个宿主计算机造成影响，并且AOF文件的体积越大，使用AOF文件来进行数据还原所需的时间就越多。 为了解决AOF文件体积膨胀的问题，Redis提供了AOF文件重写（rewrite）功能。通过该功能，Redis服务器可以创建一个新的AOF文件来替代现有的AOF文件，新旧两个AOF文件所保存的数据库状态相同，但新AOF文件不会包含任何浪费空间的冗余命令，所以新AOF文件的体积通常会比旧AOF文件的体积要小得多。 AOF文件重写的实现 AOF文件重写在子进程中进行处理，不对现有的AOF文件进行任何操作，通过读取服务器当前的数据库状态来实现。 子进程在进行AOF重写期间，服务器继续处理命令请求，造成数据状态不一致，所以引入了重写缓冲区解决此问题；Redis服务器执行完一个写命令之后，它会同时将这个写命令发送给AOF缓冲区和AOF重写缓冲区。 子进程完成AOF重写之后，会向父进程发送一个信号，父进程在接到该信号会调用一个信号处理函数（将AOF重写缓冲区中的所有内容写入到新AOF文件，对新的AOF文件进行改名，原子地覆盖现有的AOF文件）；信号处理函数执行完毕之后，父进程就可以继续像往常一样接受命令请求。 二、过期键删除策略 在设置数据库过期键的时候会指定过期时间，那么一个键过期了，何时进行删除呢？ 常见的几种删除策略： 定时删除：在设置键的过期时间的同时，创建一个定时器，键过期时立即执行对键的删除操作。 优点：对内存是最友好的 缺点：对CPU时间是最不友好的 定期删除：每隔一段时间，程序就对数据库进行一次检查，删除里面的过期键。 优点：对内存和CPU的影响可调节 缺点：如何确定删除操作的频率和时长 惰性删除：键过期后不执行删除操作，每次从键空间中获取键时，检查键是否过期，如过期则删除该键；如果没有过期则返回该键。 优点：对内存是最不友好的 缺点：对CPU时间是最友好的、不被访问的过期键永远无法删除造成内存泄漏 Redis采用的过期删除策略 Redis服务器实际使用的是惰性删除和定期删除两种策略：通过配合使用这两种删除策略，服务器可以很好地在合理使用CPU时间和避免浪费内存空间之间取得平衡。 惰性删除策略的实现 在执行get set等命令时会先调用函数expireIfNeeded对过期键进行处理 获取键的过期时间 when 如果是从库直接返回 now &gt; when 是主库，未到过期时间不做任何操作直接返回 是主库，已过期，写AOF文件--&gt;通知从库删除过期键--&gt;删除过期键 定期删除策略的实现 activeExpireCycle 循环所有数据库进行过期操作； 如果库中没有设置了过期时间的key，则跳过该库； 如果只有不到1%的slots中有过期键，停止处理，等待更好的时间进行过期删除操作； 从当前库中设置了过期时间的key中随机取指定个数（默认20）的key判断是否过期，过期则删除； 判断当前库随机取的key中，已经过期的key占比超过25%则继续循环该库进行删除操作； 当函数运行超过了指定的时间，为防止长时间占用CPU停止所有操作，记录当前循环到哪一个库（变量 current_db），下次循环从此库开始执行。 内存淘汰策略 volatile-lru -&gt; 加入键的时候如果过限，首先从设置了过期时间的键集合中驱逐最久没有使用的键 allkeys-lru -&gt; 加入键的时候，如果过限，首先通过LRU算法驱逐最久没有使用的键 volatile-lfu -&gt; 从所有配置了过期时间的键中驱逐使用频率最少的键 allkeys-lfu -&gt; 从所有键中驱逐使用频率最少的键 volatile-random -&gt; 加入键的时候如果过限，从过期键的集合中随机驱逐 allkeys-random -&gt; 加入键的时候如果过限，从所有key随机删除 volatile-ttl -&gt; 从配置了过期时间的键中驱逐马上就要过期的键 noeviction -&gt; 当内存使用超过配置的时候会返回错误，不会驱逐任何键 ","link":"https://alandelon.github.io/post/database-redis-cache/"},{"title":"Guava Cache缓存回收策略分析","content":"缓存在很多场景下都是相当有用的。例如，计算或检索一个值的代价很高，并且对同样的输入需要不止一次获取值的时候，就应当考虑使用缓存。 Guava Cache简介 优点 Guava Cache很好的封装了get、put操作。如：get数据时，获取缓存-如果没有-查询DB（get-if-absent-compute），再把查询结果放入缓存中 Guava Cache是线程安全的缓存，内部实现与ConcurrentMap相似，但增加了更多的元素失效策略。 Guava Cache提供了三种基本的缓存回收方式：基于容量回收、定时回收和基于引用回收。定时回收有两种：按照写入时间，最早写入的最先回收；按照访问时间，最早访问的最早回收。 Guava Cache提供了缓存加载命中情况的统计功能。 适用场景 缓存在很多场景下都是相当有用的。例如，计算或检索一个值的代价很高，并且对同样的输入需要不止一次获取值的时候，就应当考虑使用缓存。 相对于频繁的IO操作，使用缓存效率更高速度更快；相对于Redis等分布式缓存，Guava Cache等本地缓存减少了网络传输的消耗。 高效存储&amp;访问黄金搭档 = Guava Cache + Redis + DB Guava Cache适用于以下场景： 你愿意消耗一些内存空间来提升速度。 你预料到某些键会被查询一次以上。 缓存中存放的数据总量不会超出内存容量。 基本用法 CacheLoader 提供合理的默认方法来加载或计算与键关联的值，从LoadingCache查询的正规方式是使用get(K)方法。这个方法要么返回已经缓存的值，要么使用CacheLoader向缓存原子地加载新值。 LoadingCache&lt;String, Integer&gt; cache = CacheBuilder.newBuilder() .maximumSize(10) // 最多存放10个数据 .expireAfterWrite(10, TimeUnit.SECONDS) // 缓存10秒,从上一次数据更新开始计算 .expireAfterAccess(10, TimeUnit.SECONDS) // 缓存10秒,从上一次数据访问（读、写、更新）开始计算 .recordStats() // 开启数据统计功能 .build(new CacheLoader&lt;String, Integer&gt;() { // 数据加载，默认返回0，此处可自定义处理逻辑（如：查询DB） @Override public Integer load(String key) throws Exception { return 0; } }); @Test public void test() throws ExecutionException { Integer value = cache.get(&quot;key&quot;); System.out.println(value); } Callable 所有类型的Guava Cache，不管有没有自动加载功能，都支持get(K, Callable)方法。这个方法返回缓存中相应的值，或者用给定的Callable运算并把结果加入到缓存中。 Cache&lt;String, Integer&gt; cache2 = CacheBuilder.newBuilder() .maximumSize(10) .build(); cache2.get(&quot;test&quot;, new Callable&lt;Integer&gt;() { @Override public Integer call() throws Exception { // 这里可以做一些复杂的操作，或者针对不同key进行不同的操作 return 0; } }); Cache.put使用cache.put(key, value)方法可以直接向缓存中插入值，这会直接覆盖掉给定键之前映射的值。 缓存回收 基于容量的回收 如果要规定缓存项的数目不超过固定值，只需使用CacheBuilder.maximumSize(long)。缓存将尝试回收最近没有使用或总体上很少使用的缓存项 注：采用LRU算法回收缓存 定时回收 expireAfterAccess(long, TimeUnit)：缓存项在给定时间内没有被读/写访问，则回收。请注意这种缓存的回收顺序和基于大小回收一样。 expireAfterWrite(long, TimeUnit)：缓存项在给定时间内没有被写访问（创建或覆盖），则回收。如果认为缓存数据总是在固定时候后变得陈旧不可用，这种回收方式是可取的。 注：使用CacheBuilder构建的缓存不会&quot;自动&quot;执行清理和回收工作，也不会在某个缓存项过期后马上清理，也没有诸如此类的清理机制。相反，它会在写操作时顺带做少量的维护工作，或者偶尔在读操作时做（如果写操作实在太少） 基于引用的回收 CacheBuilder.weakKeys()：使用弱引用存储键。当键没有其它（强或软）引用时，缓存项可以被垃圾回收。 CacheBuilder.weakValues()：使用弱引用存储值。当值没有其它（强或软）引用时，缓存项可以被垃圾回收。 CacheBuilder.softValues()：使用软引用存储值。软引用只有在响应内存需要时，才按照全局最近最少使用的顺序回收。 显示清除 个别清除：Cache.invalidate(key) 批量清除：Cache.invalidateAll(keys) 清除所有缓存项：Cache.invalidateAll() 数据结构 缓存回收策略实现 GuavaCache会维护两个队列，一个writeQueue、一个accessQueue，用这两个队列来实现最近读和最近写的清除操作。AccessQueue的实现如下： static final class AccessQueue&lt;K, V&gt; extends AbstractQueue&lt;ReferenceEntry&lt;K, V&gt;&gt; { // implements Queue @Override public boolean offer(ReferenceEntry&lt;K, V&gt; entry) { // unlink // 将entry和它的前节点后节点的关联断开 connectAccessOrder(entry.getPreviousInAccessQueue(), entry.getNextInAccessQueue()); // add to tail // 将新增节点加入到队列的尾部 connectAccessOrder(head.getPreviousInAccessQueue(), entry); // entry的后节点设置为head connectAccessOrder(entry, head); return true; } @Override public ReferenceEntry&lt;K, V&gt; peek() { ReferenceEntry&lt;K, V&gt; next = head.getNextInAccessQueue(); return (next == head) ? null : next; } @Override public ReferenceEntry&lt;K, V&gt; poll() { ReferenceEntry&lt;K, V&gt; next = head.getNextInAccessQueue(); if (next == head) { return null; } remove(next); return next; } @Override @SuppressWarnings(&quot;unchecked&quot;) public boolean remove(Object o) { ReferenceEntry&lt;K, V&gt; e = (ReferenceEntry) o; ReferenceEntry&lt;K, V&gt; previous = e.getPreviousInAccessQueue(); ReferenceEntry&lt;K, V&gt; next = e.getNextInAccessQueue(); connectAccessOrder(previous, next); nullifyAccessOrder(e); return next != NullEntry.INSTANCE; } } AccessQueue重写了队列的offer 、 poll 、 peek等操作。重点关注offer()实现： 将entry和它的前节点后节点的关联断开，并且建立前节点与后节点直接的关联。 将新增entry节点加入到队列的尾部。head节点的前一个节点即尾节点。 建立尾节点与entry节点的关系，entry的后节点设置为head 如上代码中的offer操作，可以发现，AccessQueue是一个环形队列，最近更新的节点一定是在尾部的，head后面的节点一定是最不活跃的，在每一次回收数据时首先清除head后面的节点数据。 缓存设置了最大数量清除策略CacheBuilder.maximumSize(long)，会触发如下代码： 根据权重计算数量，权重可自定义实现，默认1个缓存项的权重是1，当权重超过设置的最大值则进行缓存清除。。 @GuardedBy(&quot;this&quot;) void evictEntries(ReferenceEntry&lt;K, V&gt; newest) { if (!map.evictsBySize()) { return; } drainRecencyQueue(); // If the newest entry by itself is too heavy for the segment, don't bother evicting // anything else, just that if (newest.getValueReference().getWeight() &gt; maxSegmentWeight) { if (!removeEntry(newest, newest.getHash(), RemovalCause.SIZE)) { throw new AssertionError(); } } while (totalWeight &gt; maxSegmentWeight) { ReferenceEntry&lt;K, V&gt; e = getNextEvictable(); if (!removeEntry(e, e.getHash(), RemovalCause.SIZE)) { throw new AssertionError(); } } } // 从accessQueue获取下一个应该被清理的对象 @GuardedBy(&quot;this&quot;) ReferenceEntry&lt;K, V&gt; getNextEvictable() { for (ReferenceEntry&lt;K, V&gt; e : accessQueue) { int weight = e.getValueReference().getWeight(); if (weight &gt; 0) { return e; } } throw new AssertionError(); } 每次写操作之前都会触发preWriteCleanup清理操作，如下： @GuardedBy(&quot;this&quot;) // 写操作之前执行清理操作 void preWriteCleanup(long now) { runLockedCleanup(now); } // 清理之前，先进行锁定 void runLockedCleanup(long now) { if (tryLock()) { try { // 清理弱引用已经被回收的key value数据 drainReferenceQueues(); // 清理已经过期的数据 expireEntries(now); // calls drainRecencyQueue readCount.set(0); } finally { unlock(); } } } /** * Drain the key and value reference queues, cleaning up internal entries containing garbage * collected keys or values. */ @GuardedBy(&quot;this&quot;) void drainReferenceQueues() { if (map.usesKeyReferences()) { // 清理弱引用key drainKeyReferenceQueue(); } if (map.usesValueReferences()) { // 清理弱引用value drainValueReferenceQueue(); } } @GuardedBy(&quot;this&quot;) void drainKeyReferenceQueue() { Reference&lt;? extends K&gt; ref; int i = 0; while ((ref = keyReferenceQueue.poll()) != null) { @SuppressWarnings(&quot;unchecked&quot;) ReferenceEntry&lt;K, V&gt; entry = (ReferenceEntry&lt;K, V&gt;) ref; map.reclaimKey(entry); if (++i == DRAIN_MAX) { break; } } } @GuardedBy(&quot;this&quot;) void drainValueReferenceQueue() { Reference&lt;? extends V&gt; ref; int i = 0; while ((ref = valueReferenceQueue.poll()) != null) { @SuppressWarnings(&quot;unchecked&quot;) ValueReference&lt;K, V&gt; valueReference = (ValueReference&lt;K, V&gt;) ref; map.reclaimValue(valueReference); if (++i == DRAIN_MAX) { break; } } } 定时回收策略，首先读写操作时都会把对应的entry加入到accessQueue 、 writeQueue两个队列。 if (map.recordsAccess()) { entry.setAccessTime(now); } if (map.recordsWrite()) { entry.setWriteTime(now); } accessQueue.add(entry); writeQueue.add(entry); 过期操作时遍历accessQueue 、 writeQueue两个队列，判断已经过期则执行removeEntry操作 @GuardedBy(&quot;this&quot;) void expireEntries(long now) { drainRecencyQueue(); ReferenceEntry&lt;K, V&gt; e; while ((e = writeQueue.peek()) != null &amp;&amp; map.isExpired(e, now)) { if (!removeEntry(e, e.getHash(), RemovalCause.EXPIRED)) { throw new AssertionError(); } } while ((e = accessQueue.peek()) != null &amp;&amp; map.isExpired(e, now)) { if (!removeEntry(e, e.getHash(), RemovalCause.EXPIRED)) { throw new AssertionError(); } } } 根据设置的expireAfterAccess() expireAfterWrite()来判断是否过期 /** * Returns true if the entry has expired. */ boolean isExpired(ReferenceEntry&lt;K, V&gt; entry, long now) { checkNotNull(entry); if (expiresAfterAccess() &amp;&amp; (now - entry.getAccessTime() &gt;= expireAfterAccessNanos)) { return true; } if (expiresAfterWrite() &amp;&amp; (now - entry.getWriteTime() &gt;= expireAfterWriteNanos)) { return true; } return false; } 所有缓存过期的清理策略正常是在写操作之前进行的，但是防止很久时间没有写操作，偶尔在读的时候执行清理策略，如下读的finally执行postReadCleanup() V get(K key, int hash, CacheLoader&lt;? super K, V&gt; loader) throws ExecutionException { checkNotNull(key); checkNotNull(loader); try { if (count != 0) { // read-volatile // don't call getLiveEntry, which would ignore loading values ReferenceEntry&lt;K, V&gt; e = getEntry(key, hash); if (e != null) { long now = map.ticker.read(); // 获取有效的没有过期的值 V value = getLiveValue(e, now); if (value != null) { recordRead(e, now); statsCounter.recordHits(1); // refresh的逻辑 return scheduleRefresh(e, key, hash, value, now, loader); } ValueReference&lt;K, V&gt; valueReference = e.getValueReference(); if (valueReference.isLoading()) { // 已经有一个线程在load，同步等待返回的新值 return waitForLoadingValue(e, key, valueReference); } } } // at this point e is either null or expired; return lockedGetOrLoad(key, hash, loader); } catch (ExecutionException ee) { Throwable cause = ee.getCause(); if (cause instanceof Error) { throw new ExecutionError((Error) cause); } else if (cause instanceof RuntimeException) { throw new UncheckedExecutionException(cause); } throw ee; } finally { postReadCleanup(); } } /** * Performs routine cleanup following a read. Normally cleanup happens during writes. If cleanup * is not observed after a sufficient number of reads, try cleaning up from the read thread. */ void postReadCleanup() { // DRAIN_THRESHOD = 0x3f 也就是应该读192次触发一次清理，单个segment中 if ((readCount.incrementAndGet() &amp; DRAIN_THRESHOLD) == 0) { cleanUp(); } } 注意上面的scheduleRefresh()方法，这个方法的实现决定了expireAfterWrite和refreshAfterWrite两种过期策略的区别。 两个策略都能保证在本地缓存过期时，只有一个线程去穿透缓存加载后端资源。区别是在加载资源时expireAfterWrite会让所有的线程阻塞等待新值返回,然后返回加载好的新值；而refreshAfterWrite在一个线程去拿新值的同时，其他线程先直接返回旧值，不阻塞。 V scheduleRefresh(ReferenceEntry&lt;K, V&gt; entry, K key, int hash, V oldValue, long now, CacheLoader&lt;? super K, V&gt; loader) { if (map.refreshes() &amp;&amp; (now - entry.getWriteTime() &gt; map.refreshNanos) &amp;&amp; !entry.getValueReference().isLoading()) { // 如果没有线程在load则去加载，如果已经在load则返回旧值 V newValue = refresh(key, hash, loader, true); if (newValue != null) { return newValue; } } return oldValue; } ","link":"https://alandelon.github.io/post/javatool-guava-cache/"},{"title":"剖析Guava RateLimiter限流原理","content":"限流算法主要有三种：计数器算法、漏桶算法、令牌桶算法，其中漏桶算法和令牌桶算法最为重要。 一、限流的基础算法 1.1 漏桶算法 如上图所示，假设有一个水桶，水桶有一定的容量，入水口不限速度将水全部注入到水桶中，然后水桶的出水口以一个恒定的速度将水放出，当入水口速度过大时，这个漏斗中就会积水，如果水太多了就会溢出。 优点：平滑突发请求，削减峰值 缺点：漏出的速度可能会拖慢整个系统，不能有效地利用系统的资源 1.2 令牌桶算法 如上图所示，令牌桶算法是一个存放固定容量令牌的桶，按照固定速率往桶里添加令牌。桶中存放的令牌数有最大上限，超出之后就被丢弃或者拒绝。当流量或者网络请求到达时，每个请求都要获取一个令牌，如果能够获取到，则直接处理，并且令牌桶删除一个令牌。如果获取不同，该请求就要被限流，要么直接丢弃，要么在缓冲区等待。 优点：相比漏桶算法，令牌桶算法允许一定的突发流量，但是又不会让突发流量超过我们给定的限制（单位时间窗口内的令牌数）。 二、RateLimiter限流原理 Google开源工具包Guava的RateLimiter提供了令牌桶算法实现，包含平滑突发限流(SmoothBursty)和平滑预热限流(SmoothWarmingUp)。 2.1 RateLimiter使用示例 首先通过RateLimiter.create(5)；创建一个限流器，参数代表每秒生成的令牌数，通过limiter.acquire(i)以阻塞的方式获取令牌。 RateLimiter对象可以保证1秒内不会给超过5个令牌，并且以固定速率进行放置，达到平滑输出的效果。 public void testBurstyLimiter1() { RateLimiter limiter = RateLimiter.create(5); while (true) { System.out.println(&quot;get 1 tokens: &quot; + limiter.acquire() + &quot;s&quot;); } } 输出结果： get 1 tokens: 0.0s get 1 tokens: 0.190636s get 1 tokens: 0.193058s get 1 tokens: 0.195473s get 1 tokens: 0.189132s get 1 tokens: 0.197889s get 1 tokens: 0.197371s get 1 tokens: 0.197741s get 1 tokens: 0.198837s get 1 tokens: 0.195929s RateLimiter使用令牌桶算法，会进行令牌的累积，如果获取令牌的频率比较低，则不会导致等待，直接获取令牌。 RateLimiter在没有足够令牌发放时，采用滞后处理的方式，也就是前一个请求获取令牌所需等待的时间由下一次请求来进行等待。 public void testBurstyLimiter2() { RateLimiter limiter = RateLimiter.create(2); while (true) { try { Thread.sleep(2000); } catch (Exception e) {} System.out.println(&quot;get 1 tokens: &quot; + limiter.acquire(1) + &quot;s&quot;); System.out.println(&quot;get 1 tokens: &quot; + limiter.acquire(1) + &quot;s&quot;); System.out.println(&quot;get 1 tokens: &quot; + limiter.acquire(1) + &quot;s&quot;); System.out.println(&quot;get 1 tokens: &quot; + limiter.acquire(1) + &quot;s&quot;); System.out.println(&quot;end&quot;); } } 输出结果： get 1 tokens: 0.0s get 1 tokens: 0.0s get 1 tokens: 0.0s get 1 tokens: 0.494773s end get 1 tokens: 0.0s get 1 tokens: 0.0s get 1 tokens: 0.0s get 1 tokens: 0.499717s end 2.2 RateLimiter实现原理 了解RateLimiter的基本使用方法后，我们来学习一下它的实现原理，首先来看一下类图。 RateLimiter是入口类，它提供了两套工厂方法来创建出两个子类。工厂方法会调用下面两个函数，生成RateLimiter的两个子类。 static RateLimiter create(SleepingStopwatch stopwatch, double permitsPerSecond) { RateLimiter rateLimiter = new SmoothBursty(stopwatch, 1.0 /* maxBurstSeconds */); rateLimiter.setRate(permitsPerSecond); return rateLimiter; } static RateLimiter create( SleepingStopwatch stopwatch, double permitsPerSecond, long warmupPeriod, TimeUnit unit, double coldFactor) { RateLimiter rateLimiter = new SmoothWarmingUp(stopwatch, warmupPeriod, unit, coldFactor); rateLimiter.setRate(permitsPerSecond); return rateLimiter; } RateLimiter几个重要的成员变量含义： /** * The currently stored permits. * 当前存储令牌数 */ double storedPermits; /** * The maximum number of stored permits. * 最大可存储令牌数 */ double maxPermits; /** * The interval between two unit requests, at our stable rate. E.g., a stable rate of 5 permits * per second has a stable interval of 200ms. * 添加令牌时间间隔（微秒） */ double stableIntervalMicros; /** * The time when the next request (no matter its size) will be granted. After granting a * request, this is pushed further in the future. Large requests push this further than small * requests. * 下一次请求可以获取令牌的起始时间 */ private long nextFreeTicketMicros = 0L; // could be either in the past or future 前面咱们了解过令牌桶算法原理，桶中的令牌是持续生成存放的，有请求到来时需要先从桶中拿到令牌才能开始执行，怎么持续生成令牌并放入桶中呢？ 我们一般最容易想到的就是开启一个定时任务，由定时任务持续生成令牌并放入桶中。但是这样的问题在于会极大的消耗系统资源，如某秒杀接口需要分别对每个用户做访问频率限制，那就需要每个用户开启一个定时任务，这样的开销是系统不能承受的。 RateLimiter采用的是延迟计算，等到需要拿令牌的时候再去更新桶中的令牌数量。原理就是每次调用acquire时用当前时间和nextFreeTicketMicros进行比较，根据二者的间隔和添加单位令牌的时间间隔stableIntervalMicros来刷新存储令牌数storedPermits。然后acquire会进行休眠，直到nextFreeTicketMicros，如果nextFreeTicketMicros是过去的时间则直接返回。 acquire函数实现如下： 调用reserve函数预定指定数量的令牌，并返回所需等待时间 使用SleepStopwatch进行休眠 等待时间转换为秒并返回 public double acquire(int permits) { // 计算获取令牌需等待的时间 long microsToWait = reserve(permits); // 线程sleep stopwatch.sleepMicrosUninterruptibly(microsToWait); // 等待时间转换为秒并返回 return 1.0 * microsToWait / SECONDS.toMicros(1L); } final long reserve(int permits) { checkPermits(permits); synchronized (mutex()) { return reserveAndGetWaitLength(permits, stopwatch.readMicros()); } } final long reserveAndGetWaitLength(int permits, long nowMicros) { // 预定请求令牌数量，并返回令牌可使用的时间 long momentAvailable = reserveEarliestAvailable(permits, nowMicros); // 两个时间相减，获得需要等待的时间 return max(momentAvailable - nowMicros, 0); } reserveEarliestAvailable是更新存储令牌数和下次获取令牌起始时间的关键函数，分为一下几步： 调用resync函数更新存储令牌数 计算预支付令牌所需等待的时间 更新下次获取令牌时间nextFreeTicketMicros并扣除本次消耗令牌数 final long reserveEarliestAvailable(int requiredPermits, long nowMicros) { // 根据时间刷新令牌数 resync(nowMicros); // 等待时间为上一次更新的允许获取令牌起始时间 long returnValue = nextFreeTicketMicros; // 当前存储令牌数和目标令牌数进行比较，算出可以目前即可得到的令牌数 double storedPermitsToSpend = min(requiredPermits, this.storedPermits); // 预支令牌数 = 目标令牌数 - 目前即可得到的令牌数 double freshPermits = requiredPermits - storedPermitsToSpend; // 获取目标令牌数等待时间，存储令牌中拿目前即可得令牌需要时间加上预支付令牌等待时间 long waitMicros = storedPermitsToWaitTime(this.storedPermits, storedPermitsToSpend) + (long) (freshPermits * stableIntervalMicros); try { // 更新nextFreeTicketMicros,预先支付令牌等待的时间让下一次请求来实际等待 this.nextFreeTicketMicros = LongMath.checkedAdd(nextFreeTicketMicros, waitMicros); } catch (ArithmeticException e) { this.nextFreeTicketMicros = Long.MAX_VALUE; } // 更新当前存储令牌数 this.storedPermits -= storedPermitsToSpend; return returnValue; } void resync(long nowMicros) { // nextFreeTicketMicros早于当前时间nowMicros，同步为当前时间，并刷新存储令牌数 if (nowMicros &gt; nextFreeTicketMicros) { // coolDownIntervalMicros函数获取生成令牌时间间隔 storedPermits = min(maxPermits, storedPermits + (nowMicros - nextFreeTicketMicros) / coolDownIntervalMicros()); nextFreeTicketMicros = nowMicros; } } // SmoothBursty double coolDownIntervalMicros() { return stableIntervalMicros; } // SmoothBursty long storedPermitsToWaitTime(double storedPermits, double permitsToTake) { return 0L; } 上面是平滑突发限流的实现，下面看一下加上预热缓冲期的实现原理。 * * ^ throttling * | * cold + / * interval | /. * | / . * | / . &lt;-- &quot;warmup period&quot; is the area of the trapezoid between * | / . thresholdPermits and maxPermits * | / . * | / . * | / . * stable +----------/ WARM . * interval | . UP . * | . PERIOD. * | . . * 0 +----------+-------+--------------&gt; storedPermits * 0 thresholdPermits maxPermits SmoothWarmingUp在初始阶段与SmoothBursty有点不同，SmoothWarmingUp初始storePermits = maxPermits。如上图所示，从右至左分发令牌的速率会先慢后快，一直使用permits直至storePermits减少到thresholdPermits放入token的时间便稳定下来，到达了“热状态”，之后令牌消费和SmoothBursty一样。 SmoothWarmingUp计算消耗令牌等待时间的代码如下： // SmoothWarmingUp计算等待时间就是计算上图中梯形或者矩形的面积。 long storedPermitsToWaitTime(double storedPermits, double permitsToTake) { // 当前存储令牌数超出阈值的部分 double availablePermitsAboveThreshold = storedPermits - thresholdPermits; long micros = 0; // 如果当前存储的令牌数超出阈值thresholdPermits if (availablePermitsAboveThreshold &gt; 0.0) { // 阈值右侧需被消耗的令牌数 double permitsAboveThresholdToTake = min(availablePermitsAboveThreshold, permitsToTake); /** * 阈值右侧被消耗令牌数需等待时间 = 梯形面积 * * 高是 permitsAboveThresholdToTake 即右侧需要消费的令牌数 * 底是 permitsToTime(availablePermitsAboveThreshold) * 顶是 permitsToTime(availablePermitsAboveThreshold - permitsAboveThresholdToTake) */ micros = (long) (permitsAboveThresholdToTake * (permitsToTime(availablePermitsAboveThreshold) + permitsToTime(availablePermitsAboveThreshold - permitsAboveThresholdToTake)) / 2.0); permitsToTake -= permitsAboveThresholdToTake; } // 稳定时消耗令牌等待时间 micros += (stableIntervalMicros * permitsToTake); return micros; } private double permitsToTime(double permits) { return stableIntervalMicros + permits * slope; } double coolDownIntervalMicros() { // warmuptime时间内增长的令牌数为maxPermits return warmupPeriodMicros / maxPermits; } ","link":"https://alandelon.github.io/post/javatool-guava-limit/"},{"title":"JSON常用类库介绍（二）","content":"Gson（又称Google Gson）是Google公司发布的一个开放源代码的Java库，主要用途为序列化Java对象为JSON字符串，或反序列化JSON字符串成Java对象。 一、Gson的基本用法 POJO类与JSON转换 @Data public class Sku implements Serializable { /** id 编号 */ private long skuId; /** 图片地址 */ private String imgUrl; /** 创建时间 */ private Date createTime; } 生成JSON Sku sku = new Sku(); sku.setSkuId(20190317001L); sku.setImgUrl(&quot;//img12.360buyimg.com/45ab3dd6c35d981b.jpg&quot;); sku.setCreateTime(new Date()); Gson gson = new Gson(); String json = gson.toJson(sku); // {&quot;skuId&quot;:20190317001,&quot;imgUrl&quot;:&quot;//img12.360buyimg.com/45ab3dd6c35d981b.jpg&quot;,&quot;createTime&quot;:&quot;Mar 17, 2019 11:23:20 AM&quot;} 解析JSON： Gson gson = new Gson(); String jsonString = &quot;{\\&quot;skuId\\&quot;:20190317001,\\&quot;imgUrl\\&quot;:\\&quot;//img12.360buyimg.com/45ab3dd6c35d981b.jpg\\&quot;,\\&quot;createTime\\&quot;:\\&quot;Mar 17, 2019 11:23:20 AM\\&quot;}&quot;; Sku skuFromJson = gson.fromJson(jsonString, Sku.class); 属性重命名 @SerializedName 接收数据时期望json格式： {&quot;skuId&quot;:20190317001,&quot;imgUrl&quot;:&quot;//img12.360buyimg.com/45ab3dd6c35d981b.jpg&quot;} 实际json格式： {&quot;skuId&quot;:20190317001,&quot;img_url&quot;:&quot;//img12.360buyimg.com/45ab3dd6c35d981b.jpg&quot;} 前台和后台（团队之间）数据命名格式不统一，java后台采用驼峰方法命名，前台js可能钟情于下划线方式命名；团队之间对于同一个字段的命名方式不同。这种情况下双方不肯妥协或是历史遗留问题，怎么办？ 使用@SerializedName注解对属性重命名： /** 图片地址 */ @SerializedName(&quot;img_url&quot;) private String imgUrl; 上面的问题解决了 但是由于设计不严谨，又有团队发来以下格式的json，怎么办？ {&quot;skuId&quot;:20190317001,&quot;img&quot;:&quot;//img12.360buyimg.com/45ab3dd6c35d981b.jpg&quot;} {&quot;skuId&quot;:20190317001,&quot;imgUrl&quot;:&quot;//img12.360buyimg.com/45ab3dd6c35d981b.jpg&quot;} 我们可以为POJO字段提供备选属性名 SerializedName注解提供了两个属性，上面用到了一个，还有一个属性alternate，接收一个String数组。 @SerializedName(value = &quot;imgUrl&quot;, alternate = {&quot;img&quot;, &quot;img_url&quot;}) private String imgUrl; 二、Gson泛型支持 解析Sku对象数组json串，如下： [{&quot;skuId&quot;:1,&quot;imgUrl&quot;:&quot;//img.com/1.jpg&quot;},{&quot;skuId&quot;:2,&quot;imgUrl&quot;:&quot;//img.com/2.jpg&quot;}] Gson提供了TypeToken来实现对泛型的支持，所以当我们希望使用将以上的数据解析为List时需要这样写。 Gson gson = new Gson(); String skuListJson = &quot;[{\\&quot;skuId\\&quot;:1,\\&quot;imgUrl\\&quot;:\\&quot;//img.com/1.jpg\\&quot;},{\\&quot;skuId\\&quot;:2,\\&quot;imgUrl\\&quot;:\\&quot;//img.com/2.jpg\\&quot;}]&quot;; List&lt;Sku&gt; resultSkuList = gson.fromJson(skuListJson, new TypeToken&lt;List&lt;Sku&gt;&gt;(){}.getType()); 注：TypeToken的构造方法是protected修饰的,所以上面才会写成new TypeToken&lt;List&gt;() {}.getType() 而不是 new TypeToken&lt;List&gt;().getType() 三、GsonBuilder介绍 一般情况下Gson类提供的 API已经能满足大部分的使用场景，但我们需要更特殊、更强大的功能时，这时候就引入一个新的类 GsonBuilder。 GsonBuilder是用于构建Gson实例的一个类，要想改变Gson默认的设置必须使用该类配置Gson。 基本用法 Gson gson = new GsonBuilder() // 各种配置 .create(); //生成配置好的Gson Gson在默认情况下是不动导出值null的键的，如： @Data public class Sku implements Serializable { /** id 编号 */ private long skuId; /** 图片地址 */ private String imgUrl; /** 创建时间 */ private Date createTime; } @Test public void testToJson() { Sku sku = new Sku(); sku.setSkuId(20190317001L); sku.setCreateTime(new Date()); Gson gson = new Gson(); String json = gson.toJson(sku); log.info(json); // {&quot;skuId&quot;:20190317001,&quot;createTime&quot;:&quot;Mar 17, 2019 1:11:33 PM&quot;} } 可以看出，imgUrl字段在json串中没有出现，如果接口要求没有值必须用null代替时，怎么处理？ 如下： @Test public void testToJson() { Sku sku = new Sku(); sku.setSkuId(20190317001L); sku.setCreateTime(new Date()); Gson gson = new GsonBuilder() // 序列化空值 .serializeNulls() .create(); String json = gson.toJson(sku); log.info(json); // {&quot;skuId&quot;:20190317001,&quot;imgUrl&quot;:null,&quot;createTime&quot;:&quot;Mar 17, 2019 1:15:06 PM&quot;} } POJO与JSON的字段映射规则: GsonBuilder.setFieldNamingPolicy 方法与Gson提供枚举类FieldNamingPolicy配合使用，该枚举类提供了5种实现方式分别为： FieldNamingPolicy 结果 IDENTITY {&quot;skuId&quot;:20190317001,&quot;createTime&quot;:&quot;2019-03-17&quot;} LOWER_CASE_WITH_DASHES {&quot;sku-id&quot;:20190317001,&quot;create-time&quot;:&quot;2019-03-17&quot;} LOWER_CASE_WITH_UNDERSCORES {&quot;sku_id&quot;:20190317001,&quot;create_time&quot;:&quot;2019-03-17&quot;} UPPER_CAMEL_CASE {&quot;SkuId&quot;:20190317001,&quot;CreateTime&quot;:&quot;2019-03-17&quot;} UPPER_CAMEL_CASE_WITH_SPACES {&quot;Sku Id&quot;:20190317001,&quot;Create Time&quot;:&quot;2019-03-17&quot;} Gson gson = new GsonBuilder() .setFieldNamingPolicy(FieldNamingPolicy.IDENTITY) .create(); 自定义映射规则： Gson gson = new GsonBuilder() .setFieldNamingStrategy(new FieldNamingStrategy() { @Override public String translateName(Field f) { // 实现自己的规则，规则定义可参照FieldNamingPolicy枚举类书写 return null; } }) .create(); 格式化输出、日期时间及其它： Gson gson = new GsonBuilder() // 序列化null .serializeNulls() // 设置日期时间格式，另有2个重载方法 // 在序列化和反序化时均生效 .setDateFormat(&quot;yyyy-MM-dd&quot;) // 禁此序列化内部类 .disableInnerClassSerialization() // 生成不可执行的Json（多了 )]}' 这4个字符） .generateNonExecutableJson() // 禁止转义html标签 .disableHtmlEscaping() // 格式化输出 .setPrettyPrinting() .create(); 四、过滤字段的几种方式 基于@Expose注解 @Expose 注解从名字上就可以看出是暴露的意思，所以该注解是用于对外暴露字段的。该注解必须和GsonBuilder配合使用，默认通过new Gson()的方式该注解不生效。 @Expose提供了两个属性，且都有默认值，开发者可以根据需要设置不同的值。 @Documented @Retention(RetentionPolicy.RUNTIME) @Target({ElementType.FIELD}) public @interface Expose { boolean serialize() default true; boolean deserialize() default true; } 使用方法： 简单说来就是需要导出的字段上加上**@Expose 注解，不导出的字段不加。注意是不导出的不加**。 @Expose // 默认序列化反序列化都生效 @Expose(deserialize = true,serialize = true) // 序列化和反序列化都都生效，等价于上一条 @Expose(deserialize = true,serialize = false) // 反序列化时生效 @Expose(deserialize = false,serialize = true) // 序列化时生效 @Expose(deserialize = false,serialize = false) // 和不写注解一样 Gson gson = new GsonBuilder() .excludeFieldsWithoutExposeAnnotation() .create(); 基于版本 Gson在对基于版本的字段导出提供了两个注解 @Since 和 @Until，和 GsonBuilder.setVersion(Double) 配合使用。@Since 和 @Until都接收一个Double值。 使用方法：当前版本(GsonBuilder中设置的版本) 大于等于Since的值时该字段导出，小于Until的值时该该字段导出。 class SinceUntilSample { @Since(4) public String since; @Until(5) public String until; } public void sineUtilTest(double version){ SinceUntilSample sinceUntilSample = new SinceUntilSample(); sinceUntilSample.since = &quot;since&quot;; sinceUntilSample.until = &quot;until&quot;; Gson gson = new GsonBuilder().setVersion(version).create(); System.out.println(gson.toJson(sinceUntilSample)); } // 当version &lt;4时，结果：{&quot;until&quot;:&quot;until&quot;} // 当version &gt;=4 &amp;&amp; version &lt;5时，结果：{&quot;since&quot;:&quot;since&quot;,&quot;until&quot;:&quot;until&quot;} // 当version &gt;=5时，结果：{&quot;since&quot;:&quot;since&quot;} 注：当一个字段被同时注解时，需两者同时满足条件。 基于访问修饰符 什么是修饰符? public、static 、final、private、protected 这些就是，使用方式： class ModifierSample { final String finalField = &quot;final&quot;; static String staticField = &quot;static&quot;; public String publicField = &quot;public&quot;; protected String protectedField = &quot;protected&quot;; String defaultField = &quot;default&quot;; private String privateField = &quot;private&quot;; } ModifierSample modifierSample = new ModifierSample(); Gson gson = new GsonBuilder() .excludeFieldsWithModifiers(Modifier.FINAL, Modifier.STATIC, Modifier.PRIVATE) .create(); log.info(gson.toJson(modifierSample)); // {&quot;publicField&quot;:&quot;public&quot;,&quot;protectedField&quot;:&quot;protected&quot;,&quot;defaultField&quot;:&quot;default&quot;} 基于策略（自定义规则） 基于策略是利用Gson提供的ExclusionStrategy接口，同样需要使用GsonBuilder,相关API 2个，分别是addSerializationExclusionStrategy 和addDeserializationExclusionStrategy 分别针对序列化和反序化时。这里以序列化为例。 Gson gson = new GsonBuilder() .addSerializationExclusionStrategy(new ExclusionStrategy() { @Override public boolean shouldSkipField(FieldAttributes f) { // 按属性名排除 if (&quot;skuId&quot;.equals(f.getName())){ return true; } Expose expose = f.getAnnotation(Expose.class); // 按注解排除 if (expose != null &amp;&amp; !expose.deserialize()){ return true; } return false; } @Override public boolean shouldSkipClass(Class&lt;?&gt; clazz) { // 直接排除某个类 ，return true为排除 return (clazz == int.class || clazz == Integer.class); } }) .create(); 是不是很强大，想怎么排除就怎么排除。 五、TypeAdapter TypeAdapter 是Gson提供的一个抽象类，用于接管某种类型的序列化和反序列化过程，包含两个注要方法 write(JsonWriter,T) 和 read(JsonReader) 。 /** * @author jiangjian * @date 2019/3/17 14:40 */ public class SkuAdapter extends TypeAdapter&lt;Sku&gt; { @Override public void write(JsonWriter jsonWriter, Sku sku) throws IOException { jsonWriter.beginObject(); jsonWriter.name(&quot;SKU编码&quot;).value(sku.getSkuId()); jsonWriter.name(&quot;图片地址&quot;).value(sku.getImgUrl()); jsonWriter.name(&quot;创建时间&quot;).value(DateFormatUtils.format(sku.getCreateTime(), &quot;yyyy-MM-dd&quot;)); jsonWriter.endObject(); } @Override public Sku read(JsonReader jsonReader) throws IOException { Sku sku = new Sku(); jsonReader.beginObject(); while (jsonReader.hasNext()) { switch (jsonReader.nextName()) { case &quot;skuId&quot;: sku.setSkuId(jsonReader.nextLong()); break; case &quot;img&quot;: case &quot;img_url&quot;: case &quot;imgUrl&quot;: sku.setImgUrl(jsonReader.nextString()); break; case &quot;createTime&quot;: try { sku.setCreateTime(DateUtils.parseDate(jsonReader.nextString(),&quot;yyyy-MM-dd&quot;)); } catch (ParseException e) { sku.setCreateTime(null); } break; default: } } jsonReader.endObject(); return sku; } } @Data @JsonAdapter(SkuAdapter.class) public class Sku implements Serializable { /** id 编号 */ private long skuId; /** 图片地址 */ @Expose private String imgUrl; /** 创建时间 */ private Date createTime; } @Test public void testToJson() { Sku sku = new Sku(); sku.setSkuId(20190317001L); sku.setImgUrl(&quot;//img.com/xx.jpg&quot;); sku.setCreateTime(new Date()); Gson gson = new Gson(); String json = gson.toJson(sku); log.info(json); // {&quot;SKU编码&quot;:20190317001,&quot;图片地址&quot;:&quot;//img.com/xx.jpg&quot;,&quot;创建时间&quot;:&quot;2019-03-17&quot;} } 当我们为Sku.class 注册了 TypeAdapter之后，只要是操作Sku.class，那些之前介绍的@SerializedName 、FieldNamingStrategy、Since、Until、Expos通通都黯然失色，失去了效果，只会调用我们实现的SkuTypeAdapter.write(JsonWriter, User) 方法，我想怎么写就怎么写。 Gson API Javadoc: http://www.javadoc.io/doc/com.google.code.gson/gson/2.8.5 ","link":"https://alandelon.github.io/post/javatool-gson/"},{"title":"JSON常用类库介绍（一）","content":"JSON是一种文本方式展示结构化数据的方式，从产生开始就由于其简单好用、跨平台，特别适合HTTP下数据的传输（例如现在很流行的REST）而被广泛使用。 一、JSON是什么 JSON是一种文本方式展示结构化数据的方式，从产生开始就由于其简单好用、跨平台，特别适合HTTP下数据的传输（例如现在很流行的REST）而被广泛使用。 结构与类型 两种结构：对象内的键值对集合结构和数组，对象用{}表示、内部是”key”:”value”，数组用[]表示，不同值用逗号分开 基本数值有7个： true / false / null / object / array / number / string 结构可以嵌套，进而可以用来表达复杂的数据 优点 基于纯文本，所以对于人类阅读是很友好的。 规范简单，所以容易处理，开箱即用，特别是JS类的ECMA脚本里是内建支持的，可以直接作为对象使用。 平台无关性，因为类型和结构都是平台无关的，而且好处理，容易实现不同语言的处理类库，可以作为多个不同异构系统之间的数据传输格式协议，特别是在HTTP/REST下的数据格式。 缺点 性能一般，文本表示的数据一般来说比二进制大得多，在数据传输上和解析处理上都要更影响性能。 缺乏schema，跟同是文本数据格式的XML比，在类型的严格性和丰富性上要差很多。 Google JSON风格指南 遵循好的设计与编码风格，能提前解决80%的问题: 英文版Google JSON Style Guide：https://google.github.io/styleguide/jsoncstyleguide.xml 中文版Google JSON风格指南：https://github.com/darcyliu/google-styleguide/blob/master/JSONStyleGuide.md 二、常用JSON库 Gson 项目地址：https://github.com/google/gson Gson是目前功能最全的Json解析神器，Gson当初是为因应Google公司内部需求而由Google自行研发而来，但自从在2008年五月公开发布第一版后已被许多公司或用户应用。 Gson的应用主要为toJson与fromJson两个转换函数，无依赖，不需要额外的jar，能够直接跑在JDK上。 类里面只要有get和set方法，Gson完全可以实现复杂类型的json到bean或bean到json的转换，是JSON解析的神器。 FastJson 项目地址：https://github.com/alibaba/fastjson Fastjson是一个Java语言编写的高性能的JSON处理器，由阿里巴巴公司开发。无依赖，不需要额外的jar，能够直接跑在JDK上。 FastJson在复杂类型的Bean转换Json上会出现一些问题，可能会出现引用的类型，导致Json转换出错（可通过禁止循环引用避免）。 FastJson采用独创的算法，将parse的速度提升到极致，超过所有json库。 Jackson 项目地址：https://github.com/FasterXML/jackson Jackson是当前用的比较广泛的，用来序列化和反序列化json的Java开源框架。Jackson社区比较活跃，更新速度也比较快， 从Github中的统计来看，Jackson是最流行的json解析器之一，Spring MVC的默认json解析器便是Jackson。Jackson 所依赖的jar包较少，简单易用，解析大的 json 文件速度比较快。 Json-lib 项目地址：http://json-lib.sourceforge.net/index.html json-lib是最早的也是应用广泛的json解析工具，json-lib 不好的地方是依赖于很多第三方包，对于复杂类型的转换，json-lib对于json转换成bean还有缺陷， 比如一个类里面会出现另一个类的list或者map集合，json-lib从json到bean的转换就会出现问题。json-lib在功能和性能上面都不能满足现在互联网化的需求。 序列化性能对比： 反序列化性能对比： 从上面的测试结果对比可以看出来，Fastjson速度是真的牛，没有对手。Gson在序列化次数较少时性能还不错，次数多了以后相比Fastjson、Jackson速度稍微慢些，Jackson一直表现优异，至于Json-lib大家自己看吧。 三、Fastjson API POJO与JSON转换 @Data public class Sku implements Serializable { /** id 编号 */ private long skuId; /** 图片地址 */ private String imgUrl; /** 创建时间 */ private Date createTime; } POJO序列化生成JSON: @Test public void testToJson() { Sku sku = new Sku(); sku.setSkuId(20190324001L); sku.setImgUrl(&quot;//img.com/fastjson.jpg&quot;); sku.setCreateTime(new Date()); String json = JSON.toJSONString(sku); // {&quot;createTime&quot;:1553571270775,&quot;imgUrl&quot;:&quot;//img.com/fastjson.jpg&quot;,&quot;skuId&quot;:20190324001} } JSON反序列化生成POJO: @Test public void testFromJson() { String jsonStr = &quot;{\\&quot;createTime\\&quot;:1553571270775,\\&quot;imgUrl\\&quot;:\\&quot;//img.com/fastjson.jpg\\&quot;,\\&quot;skuId\\&quot;:20190324001}&quot;; Sku sku = JSON.parseObject(jsonStr, Sku.class); } JSONObject、JSONArray与JSON转换 String jsonStr = &quot;{\\&quot;createTime\\&quot;:1553571270775,\\&quot;imgUrl\\&quot;:\\&quot;//img.com/fastjson.jpg\\&quot;,\\&quot;skuId\\&quot;:20190324001}&quot;; JSONObject jsonObject = JSON.parseObject(jsonStr); jsonObject.getLong(&quot;skuId&quot;); jsonObject.getString(&quot;imgUrl&quot;); jsonObject.getDate(&quot;createTime&quot;); ... String skuListJson = &quot;[{\\&quot;skuId\\&quot;:1,\\&quot;imgUrl\\&quot;:\\&quot;//img.com/1.jpg\\&quot;},{\\&quot;skuId\\&quot;:2,\\&quot;imgUrl\\&quot;:\\&quot;//img.com/2.jpg\\&quot;}]&quot;; JSONArray jsonArray = JSON.parseArray(skuListJson); for (Object obj : jsonArray) { JSONObject jsonObject = (JSONObject) obj; jsonObject.getLong(&quot;skuId&quot;); ... } 泛型反序列化 @Test public void testTypeReference() { String skuListJson = &quot;[{\\&quot;skuId\\&quot;:1,\\&quot;imgUrl\\&quot;:\\&quot;//img.com/1.jpg\\&quot;},{\\&quot;skuId\\&quot;:2,\\&quot;imgUrl\\&quot;:\\&quot;//img.com/2.jpg\\&quot;}]&quot;; List&lt;Sku&gt; skuList = JSON.parseObject(skuListJson, new TypeReference&lt;List&lt;Sku&gt;&gt;(){}); log.info(skuList.get(0).getImgUrl()); } 处理日期 指定日期输出格式： JSON.toJSONStringWithDateFormat(sku, &quot;yyyy-MM-dd HH:mm:ss&quot;); // {&quot;createTime&quot;:&quot;2019-03-27 15:09:25&quot;,&quot;imgUrl&quot;:&quot;//img.com/fastjson.jpg&quot;,&quot;skuId&quot;:20190324001} 使用ISO-8601日期格式： JSON.toJSONString(sku, SerializerFeature.UseISO8601DateFormat); // {&quot;createTime&quot;:&quot;2019-03-27T15:10:39.298+08:00&quot;,&quot;imgUrl&quot;:&quot;//img.com/fastjson.jpg&quot;,&quot;skuId&quot;:20190324001} 全局修改日期格式： JSON.DEFFAULT_DATE_FORMAT = &quot;yyyy-MM-dd&quot;; JSON.toJSONString(sku, SerializerFeature.WriteDateUseDateFormat); // {&quot;createTime&quot;:&quot;2019-03-27&quot;,&quot;imgUrl&quot;:&quot;//img.com/fastjson.jpg&quot;,&quot;skuId&quot;:20190324001} 反序列化能够自动识别常见日期格式： ISO-8601日期格式 yyyy-MM-dd yyyy-MM-dd HH:mm:ss yyyy-MM-dd HH:mm:ss.SSS 毫秒数字 毫秒数字字符串 定制序列化 SerializerFeature对Json格式定制 名称 含义 QuoteFieldNames 输出key时是否使用双引号,默认为true UseSingleQuotes 使用单引号而不是双引号,默认为false WriteMapNullValue 是否输出值为null的字段,默认为false WriteEnumUsingToString Enum输出name()或者original,默认为false UseISO8601DateFormat Date使用ISO8601格式输出，默认为false WriteNullListAsEmpty List字段如果为null,输出为[],而非null WriteNullStringAsEmpty 字符类型字段如果为null,输出为”“,而非null WriteNullNumberAsZero 数值字段如果为null,输出为0,而非null WriteNullBooleanAsFalse Boolean字段如果为null,输出为false,而非null SkipTransientField 如果是true，类中的Get方法对应的Field是transient，序列化时将会被忽略。默认为true SortField 按字段名称排序后输出。默认为false WriteTabAsSpecial 把\\t做转义输出，默认为false不推荐设为true PrettyFormat 结果是否格式化,默认为false WriteClassName 序列化时写入类型信息，默认为false DisableCircularReferenceDetect 消除对同一对象循环引用的问题，默认为false WriteSlashAsSpecial 对斜杠’/’进行转义 BrowserCompatible 将中文都会序列化为\\uXXXX格式，字节数会多一些，但是能兼容IE 6，默认为false WriteDateUseDateFormat 全局修改日期格式,默认为false。 DisableCheckSpecialChar 一个对象的字符串属性中如果有特殊字符如双引号，将会在转成json时带有反斜杠转移符。如果不需要转义，可以使用这个属性。默认为false BeanToArray 将对象转为array输出 JSONField注解配置 ordinal() 配置序列化和反序列化的顺序（默认按字母顺序序列化）； name() 指定字段序列化的名称； format() 指定日期格式； serialize() deserialize() 是否序列化和反序列化，默认true； label() 打标记，可定制化输出 JSON.toJSONString(sku, Labels.includes(&quot;include&quot;)); JSON.toJSONString(sku, Labels.excludes(&quot;exclude&quot;)); jsonDirect() 直接输出而不经过转译（对于存放json数据的string不做转义处理） serializeUsing() deserializeUsing() 指定序列化、反序列化使用自定义Serialize、Parser public static class Model { @JSONField(serializeUsing = ModelValueSerializer.class) public int value; } public static class ModelValueSerializer implements ObjectSerializer { @Override public void write(JSONSerializer serializer, Object object, Object fieldName, Type fieldType, int features) throws IOException { Integer value = (Integer) object; String text = value + &quot;元&quot;; serializer.write(text); } } Model model = new Model(); model.value = 100; String json = JSON.toJSONString(model); // {&quot;value&quot;:&quot;100元&quot;} alternateNames() 别名，允许多个名字的变量转成同一个属性 unwrapped() 不封箱 JSONType注解 JSONType和JSONField类似，但JSONType配置在类上，而不是field或者getter/setter方法上。不再详细介绍了。 SerializeFilter SerializeFilter是通过编程扩展的方式定制序列化。fastjson支持多种SerializeFilter，用于不同场景的定制序列化。 PropertyFilter 根据PropertyName和PropertyValue来判断是否序列化 PropertyPreFilter 根据PropertyName判断是否序列化 NameFilter 修改Key，如果需要修改Key,process返回值则可 ValueFilter 修改Value BeforeFilter 序列化时在最前添加内容 AfterFilter 序列化时在最后添加内容 1. PropertyFilter 根据PropertyName和PropertyValue来判断是否序列化 可以通过扩展实现根据object或者属性名称或者属性值进行判断是否需要序列化 public interface PropertyFilter extends SerializeFilter { boolean apply(Object object, String propertyName, Object propertyValue); } 2. PropertyPreFilter 根据PropertyName判断是否序列化 public interface PropertyPreFilter extends SerializeFilter { boolean apply(JSONSerializer serializer, Object object, String name); } 3. NameFilter 序列化时修改Key 如果需要修改Key,process返回值则可 public interface NameFilter extends SerializeFilter { String process(Object object, String propertyName, Object propertyValue); } fastjson内置一个PascalNameFilter，用于输出将首字符大写的Pascal风格。 例如： import com.alibaba.fastjson.serializer.PascalNameFilter; Object obj = ...; String jsonStr = JSON.toJSONString(obj, new PascalNameFilter()); 4. ValueFilter 序列化时修改Value public interface ValueFilter extends SerializeFilter { Object process(Object object, String propertyName, Object propertyValue); } 5. BeforeFilter 序列化时在最前添加内容 在序列化对象的所有属性之前执行某些操作,例如调用 writeKeyValue 添加内容 public abstract class BeforeFilter implements SerializeFilter { protected final void writeKeyValue(String key, Object value) { ... } // 需要实现的抽象方法，在实现中调用writeKeyValue添加内容 public abstract void writeBefore(Object object); } 6. AfterFilter 序列化时在最后添加内容 在序列化对象的所有属性之后执行某些操作,例如调用 writeKeyValue 添加内容 public abstract class AfterFilter implements SerializeFilter { protected final void writeKeyValue(String key, Object value) { ... } // 需要实现的抽象方法，在实现中调用writeKeyValue添加内容 public abstract void writeAfter(Object object); } ParseProcess ParseProcess是编程扩展定制反序列化的接口。fastjson支持如下ParseProcess： ExtraProcessor 用于处理多余的字段 ExtraTypeProvider 用于处理多余字段时提供类型信息 1. 使用ExtraProcessor 处理多余字段 public static class VO { private int id; private Map&lt;String, Object&gt; attributes = new HashMap&lt;String, Object&gt;(); public int getId() { return id; } public void setId(int id) { this.id = id;} public Map&lt;String, Object&gt; getAttributes() { return attributes;} } ExtraProcessor processor = new ExtraProcessor() { public void processExtra(Object object, String key, Object value) { VO vo = (VO) object; vo.getAttributes().put(key, value); } }; VO vo = JSON.parseObject(&quot;{\\&quot;id\\&quot;:123,\\&quot;name\\&quot;:\\&quot;abc\\&quot;}&quot;, VO.class, processor); Assert.assertEquals(123, vo.getId()); Assert.assertEquals(&quot;abc&quot;, vo.getAttributes().get(&quot;name&quot;)); 2. 使用ExtraTypeProvider 为多余的字段提供类型 public static class VO { private int id; private Map&lt;String, Object&gt; attributes = new HashMap&lt;String, Object&gt;(); public int getId() { return id; } public void setId(int id) { this.id = id;} public Map&lt;String, Object&gt; getAttributes() { return attributes;} } class MyExtraProcessor implements ExtraProcessor, ExtraTypeProvider { public void processExtra(Object object, String key, Object value) { VO vo = (VO) object; vo.getAttributes().put(key, value); } public Type getExtraType(Object object, String key) { if (&quot;value&quot;.equals(key)) { return int.class; } return null; } }; ExtraProcessor processor = new MyExtraProcessor(); VO vo = JSON.parseObject(&quot;{\\&quot;id\\&quot;:123,\\&quot;value\\&quot;:\\&quot;123456\\&quot;}&quot;, VO.class, processor); Assert.assertEquals(123, vo.getId()); Assert.assertEquals(123456, vo.getAttributes().get(&quot;value&quot;)); // value本应该是字符串类型的，通过getExtraType的处理变成Integer类型了。 更多高级功能 处理超大JSON文本，使用Stream API：https://github.com/alibaba/fastjson/wiki/Stream-api JSONPath介绍：https://github.com/alibaba/fastjson/wiki/JSONPath ","link":"https://alandelon.github.io/post/javatool-fastjson/"},{"title":"Java解析excel工具easyexcel 助你快速简单避免OOM","content":"Java解析、生成Excel比较有名的框架有Apache poi、jxl。但他们都存在一个严重的问题就是非常的耗内存，poi有一套SAX模式的API可以一定程度的解决一些内存溢出的问题，但POI还是有一些缺陷，比如07版Excel解压缩以及解压后存储都是在内存中完成的，内存消耗依然很大。easyexcel重写了poi对07版Excel的解析，能够原本一个3M的excel用POI sax依然需要100M左右内存降低到KB级别，并且再大的excel不会出现内存溢出，03版依赖POI的sax模式。在上层做了模型转换的封装，让使用者更加简单方便 easyexcel核心功能 读任意大小的03、07版Excel不会OOM 读Excel自动通过注解，把结果映射为java模型 读Excel支持多sheet 读Excel时候是否对Excel内容做trim()增加容错 写小量数据的03版Excel（不要超过2000行） 写任意大07版Excel不会OOM 写Excel通过注解将表头自动写入Excel 写Excel可以自定义Excel样式 如：字体，加粗，表头颜色，数据内容颜色 写Excel到多个不同sheet 写Excel时一个sheet可以写多个Table 写Excel时候自定义是否需要写表头 快速使用 1. JAR包依赖 使用前最好咨询下最新版，或者到mvn仓库搜索一下easyexcel的最新版 &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;easyexcel&lt;/artifactId&gt; &lt;version&gt;{latestVersion}&lt;/version&gt; &lt;/dependency&gt; 2. 读取Excel 使用easyexcel解析03、07版本的Excel只是ExcelTypeEnum不同，其他使用完全相同，使用者无需知道底层解析的差异。 无java模型直接把excel解析的每行结果以List返回 在ExcelListener获取解析结果 读excel代码示例如下： @Test public void testExcel2003NoModel() { InputStream inputStream = getInputStream(&quot;loan1.xls&quot;); try { // 解析每行结果在listener中处理 ExcelListener listener = new ExcelListener(); ExcelReader excelReader = new ExcelReader(inputStream, ExcelTypeEnum.XLS, null, listener); excelReader.read(); } catch (Exception e) { } finally { try { inputStream.close(); } catch (IOException e) { e.printStackTrace(); } } } ExcelListener示例代码如下： /* 解析监听器， * 每解析一行会回调invoke()方法。 * 整个excel解析结束会执行doAfterAllAnalysed()方法 * * 下面只是我写的一个样例而已，可以根据自己的逻辑修改该类。 * @author jipengfei * @date 2017/03/14 */ public class ExcelListener extends AnalysisEventListener { //自定义用于暂时存储data。 //可以通过实例获取该值 private List&lt;Object&gt; datas = new ArrayList&lt;Object&gt;(); public void invoke(Object object, AnalysisContext context) { System.out.println(&quot;当前行：&quot;+context.getCurrentRowNum()); System.out.println(object); datas.add(object);//数据存储到list，供批量处理，或后续自己业务逻辑处理。 doSomething(object);//根据自己业务做处理 } private void doSomething(Object object) { //1、入库调用接口 } public void doAfterAllAnalysed(AnalysisContext context) { // datas.clear();//解析结束销毁不用的资源 } public List&lt;Object&gt; getDatas() { return datas; } public void setDatas(List&lt;Object&gt; datas) { this.datas = datas; } } 有java模型映射 java模型写法如下： public class LoanInfo extends BaseRowModel { @ExcelProperty(index = 0) private String bankLoanId; @ExcelProperty(index = 1) private Long customerId; @ExcelProperty(index = 2,format = &quot;yyyy/MM/dd&quot;) private Date loanDate; @ExcelProperty(index = 3) private BigDecimal quota; @ExcelProperty(index = 4) private String bankInterestRate; @ExcelProperty(index = 5) private Integer loanTerm; @ExcelProperty(index = 6,format = &quot;yyyy/MM/dd&quot;) private Date loanEndDate; @ExcelProperty(index = 7) private BigDecimal interestPerMonth; @ExcelProperty(value = {&quot;一级表头&quot;,&quot;二级表头&quot;}) private BigDecimal sax; } @ExcelProperty(index = 3)数字代表该字段与excel对应列号做映射，也可以采用 @ExcelProperty(value = {&quot;一级表头&quot;,&quot;二级表头&quot;})用于解决不确切知道excel第几列和该字段映射，位置不固定，但表头的内容知道的情况。 @Test public void testExcel2003WithReflectModel() { InputStream inputStream = getInputStream(&quot;loan1.xls&quot;); try { // 解析每行结果在listener中处理 AnalysisEventListener listener = new ExcelListener(); ExcelReader excelReader = new ExcelReader(inputStream, ExcelTypeEnum.XLS, null, listener); excelReader.read(new Sheet(1, 2, LoanInfo.class)); } catch (Exception e) { } finally { try { inputStream.close(); } catch (IOException e) { e.printStackTrace(); } } } 带模型解析与不带模型解析主要在构造new Sheet(1, 2, LoanInfo.class)时候包含class。Class需要继承BaseRowModel暂时BaseRowModel没有任何内容，后面升级可能会增加一些默认的数据。 3. 生成Excel 每行数据是List无表头 OutputStream out = new FileOutputStream(&quot;/Users/jipengfei/77.xlsx&quot;); try { ExcelWriter writer = new ExcelWriter(out, ExcelTypeEnum.XLSX,false); //写第一个sheet, sheet1 数据全是List&lt;String&gt; 无模型映射关系 Sheet sheet1 = new Sheet(1, 0); sheet1.setSheetName(&quot;第一个sheet&quot;); writer.write(getListString(), sheet1); writer.finish(); } catch (Exception e) { e.printStackTrace(); } finally { try { out.close(); } catch (IOException e) { e.printStackTrace(); } } 每行数据是一个java模型有表头----表头层级为一 生成Excel格式如下图： 模型写法如下： public class ExcelPropertyIndexModel extends BaseRowModel { @ExcelProperty(value = &quot;姓名&quot; ,index = 0) private String name; @ExcelProperty(value = &quot;年龄&quot;,index = 1) private String age; @ExcelProperty(value = &quot;邮箱&quot;,index = 2) private String email; @ExcelProperty(value = &quot;地址&quot;,index = 3) private String address; @ExcelProperty(value = &quot;性别&quot;,index = 4) private String sax; @ExcelProperty(value = &quot;高度&quot;,index = 5) private String heigh; @ExcelProperty(value = &quot;备注&quot;,index = 6) private String last; } @ExcelProperty(value = &quot;姓名&quot;,index = 0) value是表头数据，默认会写在excel的表头位置，index代表第几列。 @Test public void test1() throws FileNotFoundException { OutputStream out = new FileOutputStream(&quot;/Users/jipengfei/78.xlsx&quot;); try { ExcelWriter writer = new ExcelWriter(out, ExcelTypeEnum.XLSX); //写第一个sheet, sheet1 数据全是List&lt;String&gt; 无模型映射关系 Sheet sheet1 = new Sheet(1, 0,ExcelPropertyIndexModel.class); writer.write(getData(), sheet1); writer.finish(); } catch (Exception e) { e.printStackTrace(); } finally { try { out.close(); } catch (IOException e) { e.printStackTrace(); } } } 每行数据是一个java模型有表头----表头层级为多层级 生成Excel格式如下图： java模型写法如下： public class MultiLineHeadExcelModel extends BaseRowModel { @ExcelProperty(value = {&quot;表头1&quot;,&quot;表头1&quot;,&quot;表头31&quot;},index = 0) private String p1; @ExcelProperty(value = {&quot;表头1&quot;,&quot;表头1&quot;,&quot;表头32&quot;},index = 1) private String p2; @ExcelProperty(value = {&quot;表头3&quot;,&quot;表头3&quot;,&quot;表头3&quot;},index = 2) private int p3; @ExcelProperty(value = {&quot;表头4&quot;,&quot;表头4&quot;,&quot;表头4&quot;},index = 3) private long p4; @ExcelProperty(value = {&quot;表头5&quot;,&quot;表头51&quot;,&quot;表头52&quot;},index = 4) private String p5; @ExcelProperty(value = {&quot;表头6&quot;,&quot;表头61&quot;,&quot;表头611&quot;},index = 5) private String p6; @ExcelProperty(value = {&quot;表头6&quot;,&quot;表头61&quot;,&quot;表头612&quot;},index = 6) private String p7; @ExcelProperty(value = {&quot;表头6&quot;,&quot;表头62&quot;,&quot;表头621&quot;},index = 7) private String p8; @ExcelProperty(value = {&quot;表头6&quot;,&quot;表头62&quot;,&quot;表头622&quot;},index = 8) private String p9; } 写Excel写法同上，只需将ExcelPropertyIndexModel.class改为MultiLineHeadExcelModel.class 一个Excel多个sheet写法 @Test public void test1() throws FileNotFoundException { OutputStream out = new FileOutputStream(&quot;/Users/jipengfei/77.xlsx&quot;); try { ExcelWriter writer = new ExcelWriter(out, ExcelTypeEnum.XLSX,false); //写第一个sheet, sheet1 数据全是List&lt;String&gt; 无模型映射关系 Sheet sheet1 = new Sheet(1, 0); sheet1.setSheetName(&quot;第一个sheet&quot;); writer.write(getListString(), sheet1); //写第二个sheet sheet2 模型上打有表头的注解，合并单元格 Sheet sheet2 = new Sheet(2, 3, MultiLineHeadExcelModel.class, &quot;第二个sheet&quot;, null); sheet2.setTableStyle(getTableStyle1()); writer.write(getModeldatas(), sheet2); //写sheet3 模型上没有注解，表头数据动态传入 List&lt;List&lt;String&gt;&gt; head = new ArrayList&lt;List&lt;String&gt;&gt;(); List&lt;String&gt; headCoulumn1 = new ArrayList&lt;String&gt;(); List&lt;String&gt; headCoulumn2 = new ArrayList&lt;String&gt;(); List&lt;String&gt; headCoulumn3 = new ArrayList&lt;String&gt;(); headCoulumn1.add(&quot;第一列&quot;); headCoulumn2.add(&quot;第二列&quot;); headCoulumn3.add(&quot;第三列&quot;); head.add(headCoulumn1); head.add(headCoulumn2); head.add(headCoulumn3); Sheet sheet3 = new Sheet(3, 1, NoAnnModel.class, &quot;第三个sheet&quot;, head); writer.write(getNoAnnModels(), sheet3); writer.finish(); } catch (Exception e) { e.printStackTrace(); } finally { try { out.close(); } catch (IOException e) { e.printStackTrace(); } } } 一个sheet中有多个表格 @Test public void test2() throws FileNotFoundException { OutputStream out = new FileOutputStream(&quot;/Users/jipengfei/77.xlsx&quot;); try { ExcelWriter writer = new ExcelWriter(out, ExcelTypeEnum.XLSX,false); //写sheet1 数据全是List&lt;String&gt; 无模型映射关系 Sheet sheet1 = new Sheet(1, 0); sheet1.setSheetName(&quot;第一个sheet&quot;); Table table1 = new Table(1); writer.write(getListString(), sheet1, table1); writer.write(getListString(), sheet1, table1); //写sheet2 模型上打有表头的注解 Table table2 = new Table(2); table2.setTableStyle(getTableStyle1()); table2.setClazz(MultiLineHeadExcelModel.class); writer.write(getModeldatas(), sheet1, table2); //写sheet3 模型上没有注解，表头数据动态传入,此情况下模型field顺序与excel现实顺序一致 List&lt;List&lt;String&gt;&gt; head = new ArrayList&lt;List&lt;String&gt;&gt;(); List&lt;String&gt; headCoulumn1 = new ArrayList&lt;String&gt;(); List&lt;String&gt; headCoulumn2 = new ArrayList&lt;String&gt;(); List&lt;String&gt; headCoulumn3 = new ArrayList&lt;String&gt;(); headCoulumn1.add(&quot;第一列&quot;); headCoulumn2.add(&quot;第二列&quot;); headCoulumn3.add(&quot;第三列&quot;); head.add(headCoulumn1); head.add(headCoulumn2); head.add(headCoulumn3); Table table3 = new Table(3); table3.setHead(head); table3.setClazz(NoAnnModel.class); table3.setTableStyle(getTableStyle2()); writer.write(getNoAnnModels(), sheet1, table3); writer.write(getNoAnnModels(), sheet1, table3); writer.finish(); } catch (Exception e) { e.printStackTrace(); } finally { try { out.close(); } catch (IOException e) { e.printStackTrace(); } } } 4. 测试数据分析 从上面的性能测试可以看出easyexcel在解析耗时上比poiuserModel模式弱了一些。主要原因是我内部采用了反射做模型字段映射，中间我也加了cache，但感觉这点差距可以接受的。但在内存消耗上差别就比较明显了，easyexcel在后面文件再增大，内存消耗几乎不会增加了。但poi userModel就不一样了，简直就要爆掉了。想想一个excel解析200M，同时有20个人再用估计一台机器就挂了。 5. 百万数据解析对比 easyexcel解析百万数据内存图如下： [外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-YVo5Q2gh-1631244928914)(https://www.itsleuth.cn/upload/2018/7/201807101724572018071110170464.png &quot;easyexcel解析百万数据内存图&quot;)] poi解析百万数据内存图如下： [外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-9Ebaubd6-1631244928915)(https://www.itsleuth.cn/upload/2018/7/201807101732472018071110170462.png &quot;poi解析百万数据内存图&quot;)] 从上面两图可以看出，easyexcel解析时内存消耗很少，最多消耗不到50M；POI解析过程中直接飘升到1.5G左右，系统内存耗尽，程序挂掉。 GitHub地址：https://github.com/alibaba/easyexcel ","link":"https://alandelon.github.io/post/javatool-001-easyexcel/"}]}